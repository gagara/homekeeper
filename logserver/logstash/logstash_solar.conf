input {
    exec {
        id => "current data"
        command => 'curl -s -X GET "http://${FRONIUS_HOST:localhost}:${FRONIUS_PORT:80}/solar_api/v1/GetInverterRealtimeData.cgi?Scope=Device&DeviceId=1&DataCollection=CommonInverterData"'
        interval => "${FRONIUS_CURR_RQ_PULL_INTERVAL:60}"
        codec => "json"
    }
    exec {
        id => "archive data"
        command => 'curl -s -X GET "http://${FRONIUS_HOST:localhost}:${FRONIUS_PORT:80}/solar_api/v1/GetArchiveData.cgi?Scope=Device&DeviceClass=Inverter&DeviceId=1&StartDate=$(date -u --date=@$(($(date -u +%s) - ${FRONIUS_ARCH_RQ_PULL_INTERVAL:300})) +%Y-%m-%dT%H:%M:%S)&EndDate=$(date -u +%Y-%m-%dT%H:%M:%S)&Channel=Current_DC_String_1&Channel=Voltage_DC_String_1"'
        interval => "${FRONIUS_ARCH_RQ_PULL_INTERVAL:300}"
        codec => "json"
    }
}

# archive: split data by channel type
filter {
    if ([Head][RequestArguments][Channel]) {
        split {
            field => "[Head][RequestArguments][Channel]"
        }
    }
}

# archive: extract timestamp offsets
filter {
    if ([Head][RequestArguments][Channel]) {
        ruby {
            code => "
                event.set('timestamp_offset_', event.get('[Body][Data][inverter/1][Data][%1$s][Values]' % [ event.get('[Head][RequestArguments][Channel]') ]).keys)
            "
        }
    }
}

# archive: split events by timestamp_offset_ array
filter {
    if ([Head][RequestArguments][Channel] and [timestamp_offset_]) {
        split {
            field => "[timestamp_offset_]"
        }
        mutate {
            convert => { "timestamp_offset_" => "integer" }
        }
    }
}

# archive: extract data for current timestamp offset and remove all others
filter {
    if ([Head][RequestArguments][Channel] and [timestamp_offset_]) {
        ruby {
            code => "
                event.set('[Body][Data][inverter/1][Unit]', event.get('[Body][Data][inverter/1][Data][%1$s][Unit]' % [ event.get('[Head][RequestArguments][Channel]') ]))
                event.set('[Body][Data][inverter/1][Value]', event.get('[Body][Data][inverter/1][Data][%1$s][Values][%2$s]' % [ event.get('[Head][RequestArguments][Channel]'), event.get('[timestamp_offset_]') ]))
            "
        }
        mutate {
            convert => { "[Body][Data][inverter/1][Value]" => "float" }
        }
        mutate {
            remove_field => [ "[Body][Data][inverter/1][Data]" ]
        }
    }
}

# archive: update @timestamp field based on current timestamp offset
filter {
    if ([Head][RequestArguments][Channel] and [Body][Data][inverter/1][Start] and [timestamp_offset_]) {
        date {
            match => [ "[Body][Data][inverter/1][Start]", "yyyy-MM-dd'T'HH:mm:ssZZ" ]
        }
        ruby {
            code => "
                event.set('timestamp_', event.get('@timestamp').to_i + event.get('timestamp_offset_'))
            "
        }
        mutate {
            convert => { "[timestamp_]" => "string" }
        }
        date {
            match => [ "[timestamp_]", "UNIX" ]
        }
    }
}

# archive: remove temporary fields
filter {
    mutate {
        remove_field => [ "[timestamp_offset_]", "[timestamp_]" ]
    }
}

output {
    elasticsearch {
        hosts => "${ELASTICSEARCH_HOST:localhost}:${ELASTICSEARCH_PORT:9200}"
        index => "logstash_solar-%{+YYYY.MM.dd}"
        template => "/usr/share/logstash/config/template_solar.json"
        template_name => "solar"
        template_overwrite => "true"
    }
}
